{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jTvSTCnhtGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"/content/gdrive/My Drive\")\n",
        "\n",
        "# Install requirements\n",
        "! pip install tensorflow_hub==0.7 tensorflow-gpu==1.15 sentencepiece\n",
        "from albert import create_pretraining_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44dGcHothtGp",
        "colab_type": "code",
        "outputId": "5eca212e-40ca-4c53-ada1-f72f0972f065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check current working directory\n",
        "os.chdir(\"NLU_Project\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/NLU_Project'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzvruD93htGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to create a raw text file to put all documents together and then \n",
        "# use function in albert to create pretraining data\n",
        "# \"CORD-19-research-challenge/comm_use_subset/comm_use_subset/pmc_json/\",\n",
        "# \"CORD-19-research-challenge/custom_license/custom_license/pmc_json/\",\n",
        "# \"CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pmc_json/\"\n",
        "txtf = open(\"raw_pdfdata.txt\",\"w+\")\n",
        "# Change data path to your data path\n",
        "data_paths = [\"CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/\", \n",
        "              \"CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/\",\n",
        "              \"CORD-19-research-challenge/custom_license/custom_license/pdf_json/\",\n",
        "              \"CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/\"]\n",
        "all_path_list = []\n",
        "# Create a function to load all data\n",
        "def load_data(data_paths, write_to_f):\n",
        "    for folder in data_paths: # All data folders\n",
        "        for f in os.listdir(folder):\n",
        "            all_path_list.append(folder+f) # keep track so we don't miss anything\n",
        "            with open(folder+f) as data:\n",
        "                samp = json.load(data)\n",
        "                # Split sentences (one sentence per line) and give a line break for each section\n",
        "                for section in samp[\"body_text\"]:\n",
        "                    sentences = section[\"text\"].split(\".\")\n",
        "                    for sen in sentences:\n",
        "                        if sen.strip():\n",
        "                            write_to_f.write(sen.strip() + \".\\n\")\n",
        "                        else:\n",
        "                            write_to_f.write(\"\\n\") # Section line break\n",
        "\n",
        "# Now we load all data\n",
        "load_data(data_paths, txtf)\n",
        "#txtf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzV1s5J-OmiF",
        "colab_type": "code",
        "outputId": "9aa9a4ca-6970-4615-aa79-687cee6d96f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# for f in os.listdir('CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/'):\n",
        "#     print(f)\n",
        "#     break\n",
        "len(all_path_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59311"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMyDfLlHhtGu",
        "colab_type": "code",
        "outputId": "238a3ac0-8be4-4cff-d817-41ca5cf16502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Ignore this part since we are using ALBERT's vocab and tokenizer\n",
        "# Create vocab for our model\n",
        "# See this page: https://github.com/kwonmha/bert-vocab-builder\n",
        "\n",
        "# ! python ./bert-vocab-builder/subword_builder.py \\\n",
        "# --corpus_filepattern \"raw_data.txt\" \\\n",
        "# --output_filename \"vocab.txt\" \\\n",
        "# --min_count 1\n",
        "\n",
        "## Or use sentencepiece(which I used)\n",
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.Train('--input=raw_pdfdata.txt --model_prefix=covid19 --vocab_size=60000 \\\n",
        "                                --pad_id=0 --unk_id=1 --eos_id=-1 --bos_id=-1 \\\n",
        "                                --control_symbols=[CLS],[SEP],[MASK] \\\n",
        "                                --user_defined_symbols=\"(,),\\\",-,.,–,£,€\" \\\n",
        "                                --shuffle_input_sentence=true --input_sentence_size=1000000 \\\n",
        "                                --character_coverage=0.99995 --model_type=unigram')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uakNo5yUhtGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create pretraining data\n",
        "! python -m albert.create_pretraining_data \\\n",
        "  --input_file \"raw_pdfdata.txt\" \\\n",
        "  --output_file \"pre_train_data\" \\\n",
        "  --vocab_file \"3/assets/30k-clean.vocab\" \\\n",
        "  --spm_model_file \"3/assets/30k-clean.model\"\\\n",
        "  --dupe_factor 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RHZZ9gwhtG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Side note\n",
        "\n",
        "# I think it's better for us to do everything on colab...\n",
        "# To download the dataset\n",
        "# Do:\n",
        "\n",
        "! pip install kaggle\n",
        "\n",
        "# Need to get a kaggle API for this to work fine\n",
        "# FYI, see https://github.com/Kaggle/kaggle-api\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download allen-institute-for-ai/CORD-19-research-challenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmjPtGpBh8Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip CORD-19-research-challenge.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axJ2v1mpmceH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python -m albert.run_pretraining \\\n",
        "    --input_file \"pre_train_data_same_v\" \\\n",
        "    --output_dir \"train.record\" \\\n",
        "    --albert_config_file \"albert_config.json\" \\\n",
        "    --albert_hub_module_handle \"https://tfhub.dev/google/albert_base/3\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_batch_size 64 \\\n",
        "    --eval_batch_size 64 \\\n",
        "    --max_seq_length 512 \\\n",
        "    --max_predictions_per_seq 20 \\\n",
        "    --optimizer 'lamb' \\\n",
        "    --learning_rate 0.00176 \\\n",
        "    --num_train_steps 125000 \\\n",
        "    --num_warmup_steps 3125 \\\n",
        "    --save_checkpoints_steps 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJLoHTq28EhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To pretrain our model on Google Cloud: below is an example usage (need to operate on google cloud shell)\n",
        "# Environment\n",
        "virtualenv cmle-env\n",
        "source cmle-env/bin/activate\n",
        "\n",
        "# Tpu\n",
        "\"config\": {\n",
        "    \"tpuServiceAccount\": \"<account>\"\n",
        "  }\n",
        "\n",
        "# Using self models\n",
        "https://cloud.google.com/tpu/docs/tutorials/bert\n",
        "\n",
        "# Upload files\n",
        "https://cloud.google.com/storage/docs/uploading-objects\n",
        "\n",
        "# Set up and training\n",
        "ctpu up --tpu-size=v3-8 \\\n",
        " --machine-type=n1-standard-8 \\\n",
        " --zone=us-central1-b \\\n",
        " --tf-version=1.15.2 \\\n",
        " --name=pretrain_ca\n",
        "\n",
        "gcloud compute ssh cov-alb-pre --zone=us-central1-b\n",
        "\n",
        "export STORAGE_BUCKET=gs://co-albert\n",
        "export TPU_NAME=cov-alb-pre\n",
        "\n",
        "git clone https://github.com/Heimine/albert.git\n",
        "\n",
        "python3 -m albert.run_pretraining \\\n",
        "    --input_file \"${STORAGE_BUCKET}/Data/pre_train_data_same_v\" \\\n",
        "    --output_dir \"${STORAGE_BUCKET}/train_output/\" \\\n",
        "    --albert_config_file \"${STORAGE_BUCKET}/Data/albert_config.json\" \\\n",
        "    --albert_hub_module_handle \"https://tfhub.dev/google/albert_base/3\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_tpu \\\n",
        "    --tpu_name \"${TPU_NAME}\" \\\n",
        "    --train_batch_size 512 \\\n",
        "    --eval_batch_size 32 \\\n",
        "    --max_seq_length 512 \\\n",
        "    --max_predictions_per_seq 20 \\\n",
        "    --optimizer 'lamb' \\\n",
        "    --learning_rate 0.00176 \\\n",
        "    --num_train_steps 100000 \\\n",
        "    --num_warmup_steps 500 \\\n",
        "    --save_checkpoints_steps 2000\n",
        "\n",
        "# If start from a checkpoint\n",
        "python3 -m albert.run_pretraining \\\n",
        "    --input_file \"${STORAGE_BUCKET}/Data/pre_train_data_same_v\" \\\n",
        "    --output_dir \"${STORAGE_BUCKET}/train_output/\" \\\n",
        "    --albert_config_file \"${STORAGE_BUCKET}/Data/albert_config.json\" \\\n",
        "    --init_checkpoint \"${STORAGE_BUCKET}/train_output/\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_tpu \\\n",
        "    --tpu_name \"${TPU_NAME}\" \\\n",
        "    --train_batch_size 512 \\\n",
        "    --eval_batch_size 32 \\\n",
        "    --max_seq_length 512 \\\n",
        "    --max_predictions_per_seq 20 \\\n",
        "    --optimizer 'lamb' \\\n",
        "    --learning_rate 0.00176 \\\n",
        "    --num_train_steps 100000 \\\n",
        "    --num_warmup_steps 500 \\\n",
        "    --save_checkpoints_steps 2000\n",
        "\n",
        "# exit\n",
        "ctpu delete --zone=us-central1-b --name=cov-alb-pre"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}